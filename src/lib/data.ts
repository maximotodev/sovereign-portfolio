// Define Types
export interface Certification {
  id: number;
  name: string;
  organization: string;
  url: string;
  date: string;
}

export interface Project {
  id: number;
  title: string;
  description: string;
  repoUrl: string;
  liveUrl: string;
  image: string;
  tags: string[];
}

export interface BlogPost {
  id: number;
  title: string;
  slug: string;
  content: string; // Markdown
  publishedDate: string;
  isPublished: boolean;
}

// DATA STORE
export const CERTIFICATIONS: Certification[] = [
  {
    id: 1,
    name: "Google IT Automation with Python",
    organization: "Coursera",
    url: "https://www.credly.com/go/yvtuplTu",
    date: "2025-02-13",
  },
  {
    id: 2,
    name: "Google AI Essentials",
    organization: "Coursera",
    url: "https://www.coursera.org/verify/JFHTU7CWBPSP/",
    date: "2025-03-05",
  },
  {
    id: 3,
    name: "Meta Back-End Development",
    organization: "Coursera",
    url: "https://www.coursera.org/verify/CLX29DGQ9THH",
    date: "2023-10-03",
  },
  {
    id: 4,
    name: "Linux and SQL",
    organization: "Coursera",
    url: "https://coursera.org/verify/Z85DMG59FUN5",
    date: "2025-06-30",
  },
  {
    id: 5,
    name: "The Frontend Developer Career Path",
    organization: "Scrimba",
    url: "https://scrimba.com/certificate/uKLBwESL/gfrontend",
    date: "2023-05-23",
  },
  {
    id: 6,
    name: "Configuration Management and the Cloud",
    organization: "Coursera",
    url: "https://coursera.org/verify/OEYQ6UYY3SUN",
    date: "2025-02-09",
  },
];

export const PROJECTS: Project[] = [
  {
    id: 1,
    title: "Tribev2",
    description:
      "A modern social media platform designed to connect communities and foster interaction. Built with a scalable architecture to handle real-time updates.",
    repoUrl: "https://github.com/maximotodev/tribev2",
    liveUrl: "https://tribev2.vercel.app",
    image: "https://i.ibb.co/Mk1C1gnP/tribev2.png",
    tags: ["Social", "Scalable"],
  },
  {
    id: 2,
    title: "Bitcoin Chatbot",
    description:
      "Interactive chatbot providing real-time Bitcoin info, price tracking, and market trends using the Gemini API.",
    repoUrl: "https://github.com/maximotodev/bitcoin-chatbot",
    liveUrl: "https://bitcoin-chatbot.netlify.app",
    image: "https://i.ibb.co/chVrPQWW/Screen-Shot-2025-07-10-at-9-10-59-PM.png",
    tags: ["AI", "Bitcoin", "Gemini"],
  },
  {
    id: 3,
    title: "Gemini Quest",
    description:
      "Interactive trivia quiz web app powered by Gemini API. Flask backend, React + Vite frontend.",
    repoUrl: "https://github.com/maximotodev/gemini-quest",
    liveUrl: "https://gemini-quest.netlify.app",
    image:
      "https://i.ibb.co/N2Z79Rvs/Screen-Shot-2025-06-11-at-12-46-24-PM.png",
    tags: ["Game", "AI", "Flask"],
  },
  {
    id: 4,
    title: "Mia Reynolds",
    description: "Professional Portfolio Website for Mia Reynolds.",
    repoUrl: "https://github.com/maximotodev/miareynolds",
    liveUrl: "https://miareynolds.vercel.app",
    image: "https://i.ibb.co/twDgyR0B/projects-mia-reynolds.png",
    tags: ["Portfolio", "Design"],
  },
];

// Note: I truncated the content for brevity in this file, but you should paste the full markdown from your JSON here.
export const POSTS: BlogPost[] = [
  {
    id: 1,
    title: "Embeddings: The Unsung Heroes of Modern AI",
    slug: "embeddings-the-unsung-heroes-of-modern-ai",
    publishedDate: "2025-07-29",
    isPublished: true,
    content:
      '## How Do Machines Understand Meaning?\r\n\r\nComputers are fundamentally good at one thing: math. They don\'t understand words, images, or concepts in the way humans do. So how is it possible for a model like ChatGPT to grasp the nuance of a poem, or for a search engine to know that "king" and "queen" are related?\r\n\r\nThe answer lies in one of the most powerful and foundational concepts in modern artificial intelligence: **embeddings**.\r\n\r\nEmbeddings are the unsung heroes of the AI world. They are the crucial bridge that translates our complex, messy, and abstract world into the structured, numerical language that machines can actually work with.\r\n\r\n---\r\n\r\n### What Exactly is an Embedding?\r\n\r\nAt its core, an **embedding is a vector**—a list of numbers—that represents a piece of data. This "piece of data" could be anything: a word, a sentence, an entire document, an image, or even a product in an e-commerce catalog.\r\n\r\nBut it\'s not just any random list of numbers. A well-trained embedding model learns to create vectors such that the **distance between them in a high-dimensional space corresponds to their semantic similarity.**\r\n\r\nLet\'s break that down with an analogy.\r\n\r\nImagine a giant, multi-dimensional library. Instead of organizing books alphabetically, we organize them by their meaning.\r\n*   Books about "kings," "queens," and "princes" would be clustered together in one section.\r\n*   Books about "apples," "oranges," and "bananas" would be in another section, far away from the royalty section.\r\n*   Interestingly, the book "Snow White" might be situated somewhere between the "royalty" section and the "fruit" section, because it\'s related to both.\r\n\r\nIn this analogy, the "location" of each book is its embedding. It\'s a set of coordinates (`[x, y, z, ...]` ) that places it in a meaningful position relative to everything else.\r\n\r\n### How are Embeddings Created?\r\n\r\nEmbeddings are learned, not programmed. They are the output of a neural network that has been trained on a massive dataset. For example, a language model might be trained to predict the next word in a sentence. In the process of learning to do this, the model\'s internal layers develop a rich numerical representation—an embedding—for each word that captures its context and meaning.\r\n\r\nThe **AI Skill Matcher** on my own portfolio uses this exact technique. I use a pre-trained **Sentence Transformer** model (`all-MiniLM-L6-v2`) to convert a user\'s query and my project descriptions into 384-dimensional vectors. By calculating the **cosine similarity** (a measure of the angle between these vectors), the system can find the projects that are most semantically similar to the query, far beyond simple keyword matching.\r\n\r\n### Why Embeddings are a Game-Changer\r\n\r\nOnce you have a way to turn anything into a meaningful vector, you unlock a vast range of AI capabilities:\r\n\r\n1.  **Semantic Search:** This is what my Skill Matcher does. Instead of searching for "python," you can search for "backend web development language," and the model knows that a project with "Django" is a highly relevant result.\r\n\r\n2.  **Recommendation Engines:** How does Netflix know you might like a certain movie? It converts your viewing history and the movies in its catalog into embeddings. It then finds the movies whose embeddings are "closest" to yours in that high-dimensional space.\r\n\r\n3.  **Classification and Clustering:** You can feed these vectors into classic machine learning models to classify text (e.g., spam vs. not spam) or cluster similar documents together without even reading them.\r\n\r\n4.  **Multimodality:** As AI models become multimodal (understanding both text and images), they learn to place the embedding for the *word* "cat" very close to the embedding for a *picture* of a cat. This is the foundation of how models like GPT-4o work.\r\n\r\n---\r\n\r\n### Conclusion\r\n\r\nEmbeddings are the quiet workhorses of the AI revolution. They are the translator that allows us to apply powerful mathematical operations to abstract concepts like language and art. The next time you use a search engine, get a product recommendation, or chat with an LLM, remember that the magic begins with these powerful, meaning-packed lists of numbers.',
  },
  {
    id: 2,
    title: "How Bitcoin Solved the Byzantine Generals' Problem",
    slug: "how-bitcoin-solved-the-byzantine-generals-problem",
    publishedDate: "2025-07-29",
    isPublished: true,
    content:
      "## A Problem of Trust in a World of Liars\r\n\r\nImagine a group of Byzantine generals surrounding an enemy city. They need to decide unanimously whether to attack or retreat. If they all attack together, they will win. If they all retreat together, they will survive to fight another day. But if some attack while others retreat, the result will be a catastrophic failure.\r\n\r\nThe challenge is that the generals can only communicate via messengers, and some of these messengers—or even some of the generals themselves—might be traitors. A traitorous general could send an \"Attack\" message to one general and a \"Retreat\" message to another, sowing chaos and ensuring defeat.\r\n\r\nThis is the **Byzantine Generals' Problem**, a classic thought experiment in computer science. At its core, it's a question of how to achieve consensus and establish a single, verifiable truth in a distributed network where some participants may be unreliable or malicious.\r\n\r\nFor decades, this was a largely unsolved problem. How could a decentralized network of computers, communicating over an untrusted channel like the internet, ever agree on a shared state of affairs without a central coordinator?\r\n\r\n---\r\n\r\n### Enter Satoshi Nakamoto\r\n\r\nIn 2008, the pseudonymous creator of Bitcoin, Satoshi Nakamoto, published a whitepaper that didn't just propose a new form of digital money; it presented an elegant, practical solution to the Byzantine Generals' Problem.\r\n\r\nBitcoin's solution isn't a single silver bullet, but a brilliant combination of four key concepts working in concert.\r\n\r\n#### 1. The Public Ledger (Blockchain)\r\n\r\nInstead of each general keeping their own private record of messages, Bitcoin proposes a **public ledger** that everyone can see. This ledger is a chain of \"blocks,\" where each block contains a list of transactions. Every participant (or \"node\") in the network keeps a full copy of this blockchain. This transparency means there's no room for private, conflicting messages. All actions are public.\r\n\r\n*   *In our analogy, this is like having a giant, public stone tablet in the middle of the generals' camp where every message is permanently carved for all to see.*\r\n\r\n#### 2. Proof-of-Work (The Cost of Lying)\r\n\r\nThis is Satoshi's most crucial innovation. To add a new block of transactions to the blockchain, a special class of nodes called \"miners\" must solve an incredibly difficult and computationally expensive mathematical puzzle. This process is known as **Proof-of-Work**.\r\n\r\nFinding the solution is hard, but verifying it is easy. The first miner to solve the puzzle gets to propose the next block and is rewarded with newly created bitcoin.\r\n\r\n*   *This is like requiring a general to perform a massive, verifiable feat of strength—like carving a complex statue—before their message can be added to the public stone tablet. It's too costly and time-consuming to create fake statues for different generals. It's much more profitable to do the work honestly and earn the reward.*\r\n\r\n#### 3. The Chain of Trust (Immutability)\r\n\r\nEach new block on the blockchain contains a cryptographic hash (a unique digital fingerprint) of the block that came before it. This creates a strong, unbroken chain. If a traitorous general tried to go back and alter a previous message on the stone tablet, it would change the tablet's fingerprint. This would break the chain, and all the other honest generals would immediately reject it as fraudulent.\r\n\r\nTo successfully rewrite history, an attacker would need to re-do the computationally expensive Proof-of-Work for the altered block and *all subsequent blocks* faster than the entire rest of the network combined—a feat considered practically impossible.\r\n\r\n#### 4. The Economic Incentive (Rewarding Honesty)\r\n\r\nFinally, Bitcoin aligns the economic incentives of the participants with the health of the network. The miners who perform the Proof-of-Work are rewarded in bitcoin for their honest participation. It is far more profitable for a miner to use their computational power to follow the rules and earn the block reward than it is to try and cheat the system, an act which would destroy the value of any bitcoin they might acquire.\r\n\r\n*   *The generals are rewarded for their honest work with gold coins that only have value if their empire is successful. Cheating leads to the empire's collapse, rendering their coins worthless.*\r\n\r\n---\r\n\r\n### Conclusion: A New Form of Trust\r\n\r\nBy combining a public ledger, a costly but verifiable work requirement, a chain of cryptographic proof, and a powerful economic incentive, Bitcoin allows a global, decentralized network of untrusting participants to agree on a single, shared history of transactions.\r\n\r\nIt allows them to coordinate a unified \"attack\" without a central commander, even in the presence of traitors. Bitcoin didn't just create digital money; it created a new form of trust, solving a problem that had stumped computer scientists for decades and unlocking the door for the entire world of decentralized technology that we see today.",
  },
  {
    id: 3,
    title: "Beyond the Hype: Three Deep Learning Trends",
    slug: "beyond-the-hype-three-deep-learning-trends-shaping-our-future",
    publishedDate: "2025-07-29",
    isPublished: true,
    content:
      "## The AI Cambrian Explosion\r\n\r\nThe world of deep learning is moving at a breakneck pace. What was state-of-the-art just a year ago is now common practice, and new architectures and capabilities are emerging constantly. It's an exciting, and sometimes bewildering, time to be a developer in this space.\r\n\r\nBeyond the general hype of \"AI,\" there are several specific, powerful trends that are driving this innovation. Here are three of the most significant trends in deep learning that are not just theoretical but are actively shaping the products and services we use every day.\r\n\r\n---\r\n\r\n### 1. The Era of Foundation Models & LLMs\r\n\r\nThe most visible trend is the dominance of **Large Language Models (LLMs)** and, more broadly, **Foundation Models**. These are massive neural networks trained on vast, internet-scale datasets.\r\n\r\n*   **What they are:** Instead of training a small model for a single task (like sentiment analysis), a foundation model like GPT-4, Llama 3, or Claude 3 is pre-trained with a general understanding of language, reasoning, and even code.\r\n*   **Why it's a trend:** This \"pre-training\" paradigm is incredibly efficient. We no longer need massive, task-specific datasets. We can take a powerful foundation model and **fine-tune** it with a relatively small amount of data to make it an expert in a specific domain, like legal contract analysis or medical diagnostics. This has democratized access to powerful AI capabilities.\r\n*   **What's next:** Expect to see more specialized, smaller, and open-source foundation models that are fine-tuned for specific industries, as well as continued research into making these models more reliable and less prone to \"hallucination.\"\r\n\r\n### 2. Multimodality: AI That Sees and Hears\r\n\r\nFor years, AI models were specialists: one model for text, another for images, a third for audio. The second major trend is **multimodality**, where a single model can understand and process information from multiple sources simultaneously.\r\n\r\n*   **What it is:** Models like OpenAI's GPT-4o or Google's Gemini can accept a combination of text, images, and audio as input and generate responses that weave these modalities together. You can show it a picture of your refrigerator and ask, \"What can I make for dinner?\"\r\n*   **Why it's a trend:** This is a crucial step towards creating more human-like AI. We experience the world through multiple senses, and models that can do the same are capable of solving much more complex, real-world problems. This is powering everything from advanced accessibility tools that describe the world to visually impaired users to interactive design assistants.\r\n*   **What's next:** The next frontier is video. Models that can understand the temporal context of video streams will unlock a new wave of applications in robotics, autonomous systems, and content creation.\r\n\r\n### 3. Efficient AI: Doing More with Less\r\n\r\nWhile massive models grab the headlines, a powerful counter-trend is the drive for **AI efficiency**. As models become larger and more expensive to train and run, the need for smaller, faster, and more accessible models has become critical.\r\n\r\n*   **What it is:** This involves a suite of techniques like **quantization** (reducing the precision of the model's weights), **pruning** (removing unnecessary neural connections), and **knowledge distillation** (training a small \"student\" model to mimic a large \"teacher\" model).\r\n*   **Why it's a trend:** Efficient AI is what makes it possible to run powerful models on everyday devices like smartphones and laptops, rather than relying on massive cloud servers. This is essential for applications that require low latency, privacy (keeping data on-device), and offline functionality. The AI Skill Matcher on this very portfolio uses a small, efficient Sentence Transformer model that is perfect for its task without requiring a GPU.\r\n\r\n*   **What's next:** We are seeing the rise of \"Small Language Models\" (SLMs) like Microsoft's Phi-3, which can achieve remarkable performance while being small enough to run locally. This trend will bring powerful AI capabilities to edge devices, from cars to smart glasses.\r\n\r\n---\r\n\r\n### Conclusion\r\n\r\nThese three trends—Foundation Models, Multimodality, and Efficiency—are not mutually exclusive. They are intertwined forces pushing the boundaries of what's possible. The future of AI is not just about building the biggest models, but about building the smartest, most versatile, and most accessible ones. It's a future I'm excited to be building.",
  },
  {
    id: 4,
    title: "Building My Interactive Portfolio",
    slug: "building-my-interactive-portfolio-a-deep-dive-into-the-tech-stack",
    publishedDate: "2025-07-29",
    isPublished: true,
    content:
      "## From Static Page to Dynamic Application\r\n\r\nWhen I decided to build my personal portfolio, I knew I wanted to create more than just a static HTML page. I wanted to build a living application—a testament to my skills as a full-stack developer that could dynamically update, showcase my work in interesting ways, and integrate with the modern, decentralized web. This post is a deep dive into the architecture and technology choices behind the very site you're reading this on.\r\n\r\n---\r\n\r\n### The Core Architecture: A Headless Approach\r\n\r\nI chose a modern, decoupled architecture with a distinct frontend and backend. This is a standard pattern for scalable web applications and offers several key advantages:\r\n\r\n1.  **Flexibility:** The frontend and backend can be developed, deployed, and scaled independently.\r\n2.  **Specialization:** It allows for using the best tool for the job—Python/Django for its robust backend capabilities and React for its rich, interactive user interfaces.\r\n3.  **API-First Design:** The entire application is powered by a RESTful API, a crucial skill in today's development landscape.\r\n\r\nHere's a high-level overview:\r\n\r\n- **Frontend:** A static single-page application (SPA) built with **React** and **Vite**, hosted on **Vercel** for global CDN delivery.\r\n- **Backend:** A REST API built with **Django** and **Django Rest Framework**, hosted on **Render**.\r\n- **Database:** A managed **PostgreSQL** instance, also on **Render**, serving as the single source of truth for my projects, certifications, and these blog posts.\r\n\r\n---\r\n\r\n### Key Features and the Tech Behind Them\r\n\r\n#### 1. The AI Skill Matcher\r\n\r\nThis is my favorite feature. Instead of just listing my projects, I wanted to create a tool for recruiters and clients. You can paste a job description or a list of keywords, and the backend uses a **Sentence Transformer model (`all-MiniLM-L6-v2`)** to perform a semantic search.\r\n\r\n*   **How it Works:** The model converts both the user's query and my project descriptions into high-dimensional vectors (embeddings). It then uses **Cosine Similarity** to find the projects that are most semantically related, even if the exact keywords don't match. This demonstrates a practical application of modern NLP techniques.\r\n\r\n#### 2. Nostr & Bitcoin Integration\r\n\r\nAs a believer in decentralization, I wanted to connect my portfolio to the open web.\r\n\r\n*   **Nostr Profile & Notes:** The backend includes a service that connects to public Nostr relays using the `pynostr` library. It fetches my latest profile data (a `kind:0` event) and my most recent public note (a `kind:1` event) and caches the results. This makes my portfolio a live reflection of my activity on the network.\r\n\r\n*   **Bitcoin Tipping:** The \"Tip Me\" button is powered by a backend wallet created with `bitcoinlib`. The backend generates a persistent SegWit address, which is then displayed in a modal on the frontend with a QR code. This provides a universal way for anyone to send an on-chain Bitcoin tip.\r\n\r\n#### 3. Dynamic Content Management\r\n\r\nAll the content you see—projects, certifications, and this very blog post—is managed through the built-in **Django Admin Panel**. I can add, edit, and delete content from a secure interface without ever needing to redeploy the application. This is the power of a true CMS-backed architecture.\r\n\r\n---\r\n\r\n### What I Learned\r\n\r\nBuilding this project was an incredible learning experience, especially the deployment process. Navigating the resource constraints of free-tier hosting, debugging CORS and `DisallowedHost` errors, and ensuring a smooth CI/CD pipeline between GitHub, Render, and Vercel provided invaluable real-world DevOps experience.\r\n\r\nThis portfolio is a project in itself, and I'm excited to continue adding features to it. Thanks for reading!",
  },
];
